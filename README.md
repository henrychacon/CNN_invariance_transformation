# CNN features extraction display

In this project, the parallel GPU process included in the Tourch package is used to train a traditional LeNet-5 architecture proposed by LeCun in 1989:

![image1](https://github.com/henrychacon/CNN_invariance_transformation/blob/master/images/convolution_cnnlecun.png)

Kernel parameters for the two convolution layers and the activation layer for an input image are displayed:

![image2](https://github.com/henrychacon/CNN_invariance_transformation/blob/master/images/Num7.png)
![image3](https://github.com/henrychacon/CNN_invariance_transformation/blob/master/images/kernel1_7.png)

The feature extraction for each of the labels is collected and display as an interval image with min and maximum as the limit. That signal is contrasted with a rotated image and with label suggested by the full connected layer.

![image4](https://github.com/henrychacon/CNN_invariance_transformation/blob/master/images/Num7Rotated.png)

![image5](https://github.com/henrychacon/CNN_invariance_transformation/blob/master/images/Sequence7rotated.png)

In red, the signal generated by the rotated image of the number 7 and in green, the expected signal interval for that label. The DNN network suggested as output the label 6, so in the below figure, the rotated signal is presented in the signal sequence for the number 6. According to both figures, it seems that the rotated figure belongs to the second sequence.

![image6](https://github.com/henrychacon/CNN_invariance_transformation/blob/master/images/Sequence6rotated.png)
